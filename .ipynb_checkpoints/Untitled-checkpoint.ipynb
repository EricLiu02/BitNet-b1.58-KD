{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab8ed28b-7b26-4acb-8dc3-9765d4d651fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPUs detected: {gpus}\")\n",
    "    # Optionally, set memory growth to prevent TensorFlow from allocating all GPU memory\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPUs detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b29b48-d86a-4bc7-a303-67f34e7a87da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"Greetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm\"}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5887ef5f-7270-4f37-b472-3b86b6df1ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  3575,  1881,  1426,  1244, 29889, 29889, 29889, 29889, 29889,\n",
      "         29889, 29871, 29871, 29889, 29871, 29871, 29871, 29871, 29871, 29871,\n",
      "         29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871,\n",
      "         29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871,\n",
      "         29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, PreTrainedTokenizerFast\n",
    "\n",
    "# Load tokenizer from local file\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"Bitnet B1 Tokenizer.json\")\n",
    "\n",
    "# Load model from Hugging Face\n",
    "model = AutoModelForCausalLM.from_pretrained(\"1bitLLM/bitnet_b1_58-large\", trust_remote_code=True)\n",
    "model.to('cuda')\n",
    "\n",
    "# Create the text generation pipeline with both model and tokenizer\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define input messages in plain text (since `pipeline` expects raw strings, not message objects)\n",
    "user_input = \"Who are you?\"\n",
    "\n",
    "inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
    "inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=50,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "\n",
    "# Print the result\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce5660-5494-413c-a3f9-8a4cced8e1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
